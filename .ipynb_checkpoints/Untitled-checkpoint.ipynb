{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "738f8dcb-58b1-4858-b1fc-0194027ef9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_qwen2_5_dymem_infer.py\n",
    "import os\n",
    "os.environ[\"TORCH_COMPILE_DISABLE\"] = \"1\"\n",
    "os.environ[\"PYTORCH_CUDA_GRAPH_DISABLE\"] = \"1\"\n",
    "from pickle import FALSE\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from dymem.transformer.qwen2_dymem import register_customized_qwen2, Qwen2Config, Qwen2ForCausalLM\n",
    "from dymem.utils import CacheWithMem\n",
    "register_customized_qwen2(exist_ok=True)\n",
    "device = torch.device(os.environ.get(\"CUDA_DEVICE\", \"cuda:4\") if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2de6d3a8-9f12-45d9-8275-7191df0b2564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57697ac220634c86b9004ab42f6eeb82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_DIR = \"/home/xiaoyonggao/dymem/Exp_model/qwen3B-inst-simpC4B1-res-new\"\n",
    "# MODEL_DIR = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_DIR, dtype=torch.bfloat16).to(device)\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True)\n",
    "model.eval()\n",
    "# ==== 6) Tokenizer ====\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "    tok.pad_token_id = tok.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caaa2f22-b253-4b84-82e1-120bd3dcd153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_token_label(token: str, max_len: int = 24) -> str:\n",
    "    if token is None:\n",
    "        return \"\"\n",
    "    t = token\n",
    "    if t == \" \":\n",
    "        t = \"␠\"\n",
    "    if len(t) > 0 and t[0] in (\"▁\", \"Ġ\"):\n",
    "        t = \"␠\" + t[1:]\n",
    "    if t == \"\\n\":\n",
    "        t = \"\\\\n\"\n",
    "    if t == \"\\t\":\n",
    "        t = \"\\\\t\"\n",
    "    if max_len is not None and len(t) > max_len:\n",
    "        t = t[: max_len - 1] + \"…\"\n",
    "    return t\n",
    "\n",
    "\n",
    "def _decode_token_piece(tok, token_id: int) -> str:\n",
    "    try:\n",
    "        s = tok.decode([int(token_id)], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "    except TypeError:\n",
    "        s = tok.decode([int(token_id)], skip_special_tokens=False)\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    return s\n",
    "\n",
    "\n",
    "def _print_tokens_around_mem(tok, input_ids, sink_len: int, sliding_window: int, radius: int = 24):\n",
    "    ids = input_ids.detach().cpu().tolist()\n",
    "    tokens = tok.convert_ids_to_tokens(ids, skip_special_tokens=False)\n",
    "\n",
    "    if sliding_window is None:\n",
    "        sliding_window = 0\n",
    "    sliding_window = int(sliding_window)\n",
    "    sink_len = int(sink_len)\n",
    "\n",
    "    if sliding_window > 0:\n",
    "        window_start = max(sink_len + 1, len(tokens) - sliding_window)\n",
    "        window_tokens = tokens[window_start:]\n",
    "        window_ids = ids[window_start:]\n",
    "    else:\n",
    "        window_start = sink_len + 1\n",
    "        window_tokens = []\n",
    "        window_ids = []\n",
    "\n",
    "    start = max(0, sink_len - radius)\n",
    "    end = sink_len + radius + 1\n",
    "\n",
    "    print(\"\\n=== TOKENS AROUND [mem] (key index space) ===\")\n",
    "    print(f\"sink_len={sink_len}  sliding_window={sliding_window}  input_len={len(tokens)}  window_start={window_start}\")\n",
    "    for key_idx in range(start, end):\n",
    "        if key_idx < sink_len:\n",
    "            token = tokens[key_idx] if key_idx < len(tokens) else \"\"\n",
    "            token_id = ids[key_idx] if key_idx < len(ids) else None\n",
    "            src_pos = key_idx\n",
    "        elif key_idx == sink_len:\n",
    "            token = \"[mem]\"\n",
    "            token_id = None\n",
    "            src_pos = None\n",
    "        else:\n",
    "            j = key_idx - (sink_len + 1)\n",
    "            token = window_tokens[j] if 0 <= j < len(window_tokens) else \"\"\n",
    "            token_id = window_ids[j] if 0 <= j < len(window_ids) else None\n",
    "            src_pos = (window_start + j) if 0 <= j < len(window_tokens) else None\n",
    "\n",
    "        decoded = \"\" if token_id is None else _decode_token_piece(tok, token_id)\n",
    "        display = token if decoded == \"\" else decoded\n",
    "        src = \"None\" if src_pos is None else str(src_pos)\n",
    "        print(f\"key[{key_idx:5d}]  src[{src:>5}]  {_format_token_label(display)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83955e27-502d-4802-b4a6-ddbefc9462e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,\n",
       " '111 222 333 444 555 666 777 888 999 100 111 222 333 444 555 666 777 888 9111222 112 Mem:[In the summer of 1995, my friend Robert Morris and I started a startup called Viaweb. Our plan was to write software that would let end users build stores. What was novel about this software, at the time, was that it ran on our server, using ordinary Web pages as the interface.”]22 333 444 555 666 777 888 999 100111 222 333 333 444 555 666 777 888 999 100111 222 333 444 555 666 777 888 999 100111 222 333 444 555 666 777 888 999 100111 222 333 444 555 666 777 888 999 100111 222 333 444 555 666 777 888 999 100111 222 333 444 555 666 777 888 999 100111 222 333 444 555 666 777 888 999 100111 222 333 444 555 666 777 888 999 100111 222 333 444 555 666 777 888 999 100111 222 333 444 555 666 777 888 999 100111 222 333 444 555 666 777 888 999 100111 222 333 444 555 666 777 888 999 100111 222 333 444 555 666 777 888 999 100111 222 333 444 555 666 777 888 999 100111 222 333 444 555 666 777 888 999 100111 222 ')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/home/xiaoyonggao/dymem/examples/scripts/test4.txt', 'r') as f:\n",
    "    prompt = f.read()\n",
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": \"You are Qwen-Compressor, created by yg Xiao. You will use the compressed mem token to help you answer the user's question. You are a helpful assistant.\"},\n",
    "  {\"role\": \"user\", \"content\": f\"{prompt}\"}\n",
    "]\n",
    "# inputs = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = \"The mem context is: Bake me a pie or go away,\" I've literally had my son say that to me a few times. So, what was I\"\n",
    "inputs = tok(inputs, return_tensors=\"pt\").to(device)\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "input_len,messages[1]['content'][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a335a72c-b809-48f0-80ee-09283f20f8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TOKENS AROUND [mem] (key index space) ===\n",
      "sink_len=128  sliding_window=3000  input_len=20  window_start=129\n",
      "key[  118]  src[  118]  \n",
      "key[  119]  src[  119]  \n",
      "key[  120]  src[  120]  \n",
      "key[  121]  src[  121]  \n",
      "key[  122]  src[  122]  \n",
      "key[  123]  src[  123]  \n",
      "key[  124]  src[  124]  \n",
      "key[  125]  src[  125]  \n",
      "key[  126]  src[  126]  \n",
      "key[  127]  src[  127]  \n",
      "key[  128]  src[ None]  [mem]\n",
      "key[  129]  src[ None]  \n",
      "key[  130]  src[ None]  \n",
      "key[  131]  src[ None]  \n",
      "key[  132]  src[ None]  \n",
      "key[  133]  src[ None]  \n",
      "key[  134]  src[ None]  \n",
      "key[  135]  src[ None]  \n",
      "key[  136]  src[ None]  \n",
      "key[  137]  src[ None]  \n",
      "key[  138]  src[ None]  \n",
      "\n",
      "=== GENERATION ===\n",
      "The mem context is: Bake me a pie or go away,\" I've literally had my son   12 years old,   tell me that. I'm not sure if he's just being a smart-aleck   or if he really believes that. I have to say, I don't think it's funny. It's not like I'm asking him to do something that's going to be hard for him. I'm asking him to do something that he can do. I'm asking him to help me make a pie. I'm not asking him to build the Eiffel Tower. I'm not asking him to clean his room. I'm not asking him to do anything that's going to be difficult for him\n",
      "\n",
      "\n",
      "输入长度:20 ｜ 输出长度：148\n"
     ]
    }
   ],
   "source": [
    "# model.config.sliding_window = input_len-128-65\n",
    "model.config.sliding_window = 3000\n",
    "with torch.no_grad():\n",
    "    past_key_values = CacheWithMem(model.config, dtype=torch.bfloat16, device=device)\n",
    "    _print_tokens_around_mem(\n",
    "        tok,\n",
    "        inputs[\"input_ids\"][0],\n",
    "        sink_len=getattr(model.config, \"num_attn_sinks\", 128),\n",
    "        sliding_window=getattr(model.config, \"sliding_window\", None),\n",
    "        radius=10,\n",
    "    )\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        num_beams=1,\n",
    "        use_cache=True,\n",
    "        do_sample=False,\n",
    "        # temperature=0.7,\n",
    "        past_key_values=past_key_values,\n",
    "    )[0]\n",
    "    # pred = tok.decode(output[input_len:], skip_special_tokens=True)\n",
    "    pred = tok.decode(output[:], skip_special_tokens=True)\n",
    "print(f\"\\n=== GENERATION ===\\n{pred}\\n\")\n",
    "print(f\"\\n输入长度:{input_len} ｜ 输出长度：{output.shape[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27bf3d85-977f-4739-ac22-71d5af934345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cross-Entropy Loss: 1.0703\n",
      "Perplexity: 2.9219\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "context = \"\"\"In the summer of 1995, my friend Robert Morris and I started a startup called Viaweb. Our plan was to write software that would let end users build stores. What was novel about this software, at the time, was that it ran on our server, using ordinary Web pages as the interface.\n",
    "\"\"\"\n",
    "\n",
    "target = \"\"\"In the summer of 1995, my friend Robert Morris and I started a startup called Viaweb. Our plan was to write software  that would let people build their own web sites. We were inspired by the success of Apple's Macintosh computer, which made it easy for people to create their own documents. We thought we could do the same thing for web sites. We hired a programmer named David Filo, who had written a program called Yahoo! as an undergraduate at Stanford University. We also hired a designer named Dan Kottke, who had worked on the Macintosh operating system. We raised $2 million in venture capital and launched our product in 1996. We sold Viaweb to Yahoo! in 1998 for $3\n",
    "\"\"\"\n",
    "\n",
    "# tokenize\n",
    "context_ids = tok(context, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "target_ids = tok(target, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "# concat context + target\n",
    "input_ids = torch.cat([context_ids, target_ids], dim=1)\n",
    "\n",
    "# labels: mask context\n",
    "labels = input_ids.clone()\n",
    "labels[:, :context_ids.size(1)] = -100\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# shift for causal LM\n",
    "shift_logits = logits[:, :-1, :].contiguous()\n",
    "shift_labels = input_ids[:, 1:].contiguous()\n",
    "\n",
    "# mask context tokens\n",
    "shift_labels[:, :context_ids.size(1)-1] = -100\n",
    "\n",
    "# compute loss\n",
    "loss = F.cross_entropy(\n",
    "    shift_logits.view(-1, shift_logits.size(-1)),\n",
    "    shift_labels.view(-1),\n",
    "    ignore_index=-100,\n",
    "    reduction=\"mean\"\n",
    ")\n",
    "\n",
    "ppl = torch.exp(loss)\n",
    "\n",
    "print(f\"Average Cross-Entropy Loss: {loss.item():.4f}\")\n",
    "print(f\"Perplexity: {ppl.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86ca675-d9a5-4d99-8079-1cd607f83460",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in past_key_values.mem_hidden:\n",
    "    print(i.norm(dim=1,p=1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe7b0a0-02e2-45d3-ad93-cf7a6926e6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "    \n",
    "mem_hidden_shape = (1, 1, -1, 128)\n",
    "pos_emb = model.model.rotary_emb(past_key_values.mem_hidden[0],torch.tensor([[128]]).cuda())\n",
    "for l,i in enumerate(past_key_values.mem_bank):\n",
    "    # print(i.norm(dim=2,p=2).item())\n",
    "    q_mem = model.model.layers[0].self_attn.q_proj(i).view(mem_hidden_shape).transpose(1, 2)\n",
    "    k_mem = model.model.layers[0].self_attn.k_proj(i).view(mem_hidden_shape).transpose(1, 2)\n",
    "    v_mem = model.model.layers[0].self_attn.v_proj(i).view(mem_hidden_shape).transpose(1, 2)\n",
    "    q_mem,k_mem = apply_rotary_pos_emb(q_mem,k_mem,*pos_emb)\n",
    "    k_mem_norm = k_mem.norm(dim=3,p=1).mean().item()\n",
    "    v_mem_norm = v_mem.norm(dim=3,p=1).mean().item()\n",
    "    k_nrom = past_key_values.layers[l].keys[:,:,-1:,:].norm(dim=3,p=1).mean().item()\n",
    "    v_nrom = past_key_values.layers[l].values[:,:,-1:,:].norm(dim=3,p=1).mean().item()\n",
    "    print(f\"[layer {l}]:V:{{ v_mem:{v_mem_norm}\\t,v_normal:{v_nrom} }} \\t|\\tK{{k_mem:{k_mem_norm}\\t,k_normal:{k_nrom} }}\")\n",
    "# past_key_values.layers[0].values[:,:,-1:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede2b828-3bce-4b2a-af1a-d1b5d7552436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def analyze_mem_bank_logits(model, tokenizer, mem_bank, top_k=10):\n",
    "    \"\"\"\n",
    "    对 mem_bank 进行 Logit Lens 分析\n",
    "    mem_bank: shape [batch_size, seq_len, hidden_size] 或 [batch_size, hidden_size]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 1. 必须先经过模型最后的 Layer Norm 层，否则分布会偏移\n",
    "        # Qwen2 的结构通常是 model.model.norm\n",
    "        hidden_states = model.model.norm(mem_bank)\n",
    "        \n",
    "        # 2. 映射到词表空间\n",
    "        logits = model.lm_head(hidden_states)\n",
    "        \n",
    "        # 3. 计算概率分布\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # 4. 获取 Top-K 结果\n",
    "        top_probs, top_indices = torch.topk(probs, k=top_k, dim=-1)\n",
    "        \n",
    "        # 打印结果\n",
    "        # 假设我们只看第一个 batch 的最后一个 token (如果是压缩后的 single vector)\n",
    "        if len(top_indices.shape) == 3:\n",
    "            last_token_indices = top_indices[0, -1]\n",
    "            last_token_probs = top_probs[0, -1]\n",
    "        else:\n",
    "            last_token_indices = top_indices[0]\n",
    "            last_token_probs = top_probs[0]\n",
    "\n",
    "        print(f\"{'Rank':<5} | {'Token ID':<10} | {'Token String':<20} | {'Probability'}\")\n",
    "        print(\"-\" * 60)\n",
    "        for i in range(top_k):\n",
    "            token_id = last_token_indices[i].item()\n",
    "            token_str = tokenizer.decode([token_id])\n",
    "            prob = last_token_probs[i].item()\n",
    "            print(f\"{i+1:<5} | {token_id:<10} | {token_str:<20} | {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405b7eb0-79ee-400f-90f4-20e85d2ff053",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_mem_bank_logits(model, tok, past_key_values.mem_hidden[33],top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4178d00-71d0-42bd-a630-8d888c8987c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cosine_sim(target_words, mem_bank, model, tokenizer):\n",
    "    embeddings = model.get_input_embeddings().weight\n",
    "    # 归一化 mem_bank\n",
    "    mem_norm = model.model.norm(mem_bank.view(1, -1))\n",
    "    mem_norm = torch.nn.functional.normalize(mem_norm,p=2,dim=1)\n",
    "    for word in target_words:\n",
    "        token_id = tokenizer.encode(word, add_special_tokens=False)[0]\n",
    "        word_embedding = embeddings[token_id].view(1, -1)\n",
    "        word_norm = torch.nn.functional.normalize(word_embedding, p=2, dim=1)\n",
    "        \n",
    "        sim = torch.mm(mem_norm, word_norm.T).item()\n",
    "        print(f\"Cosine Similarity with '{word}': {sim:.4f}\")\n",
    "check_cosine_sim([\"1995\",\"Amazon\",\"Morris\",\"Lisp\", \"startup\", \"software\", \"Viaweb\", \"the\", \"and\"], past_key_values.mem_hidden[35], model, tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabc39d3-e022-4b2e-bd43-9b5862a69e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def plot_semantic_evolution(target_words, mem_hidden_list, model, tokenizer):\n",
    "    \"\"\"\n",
    "    绘制语义随层数变化的演化轨迹\n",
    "    mem_hidden_list: 包含每层 mem_hidden 的列表 (例如 past_key_values.mem_hidden)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    embeddings = model.get_input_embeddings().weight\n",
    "    results = []\n",
    "\n",
    "    # 1. 预计算每个单词的归一化 Embedding\n",
    "    word_vecs = {}\n",
    "    for word in target_words:\n",
    "        token_ids = tokenizer.encode(word, add_special_tokens=False)\n",
    "        if not token_ids: continue\n",
    "        token_id = token_ids[0]\n",
    "        # L2 归一化词向量\n",
    "        word_vec = torch.nn.functional.normalize(embeddings[token_id].view(1, -1), p=2, dim=1)\n",
    "        word_vecs[word] = word_vec\n",
    "\n",
    "    # 2. 逐层计算相似度\n",
    "    for i, mem_bank in enumerate(mem_hidden_list):\n",
    "        with torch.no_grad():\n",
    "            # 必须经过 RMSNorm 映射到解码流形\n",
    "            mem_hidden = model.model.norm(mem_bank.view(1, -1))\n",
    "            # L2 归一化隐藏状态\n",
    "            mem_unit = torch.nn.functional.normalize(mem_hidden, p=2, dim=1)\n",
    "            \n",
    "            for word, word_unit in word_vecs.items():\n",
    "                sim = torch.mm(mem_unit, word_unit.T).item()\n",
    "                results.append({\"Layer\": i, \"Word\": word, \"Similarity\": sim})\n",
    "\n",
    "    # 3. 转换为 DataFrame 并绘图\n",
    "    df = pd.DataFrame(results)\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    \n",
    "    # 区分关键词和虚词（可选，为了视觉更清晰）\n",
    "    palette = sns.color_palette(\"husl\", len(target_words))\n",
    "    ax = sns.lineplot(data=df, x=\"Layer\", y=\"Similarity\", hue=\"Word\", palette=palette, linewidth=2, marker='o', markersize=4)\n",
    "    \n",
    "    # 4. 标注关键转折层\n",
    "    plt.axvline(x=33, color='red', linestyle='--', alpha=0.6, label='Layer 33 (Peak)')\n",
    "    plt.text(33.5, df['Similarity'].max()*0.9, 'Semantic Peak', color='red', fontweight='bold')\n",
    "    \n",
    "    plt.title(\"Semantic Evolution of Memory Token Across Layers\", fontsize=16, fontweight='bold')\n",
    "    plt.xlabel(\"Layer Index\", fontsize=12)\n",
    "    plt.ylabel(\"Cosine Similarity (L2 Normalized)\", fontsize=12)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Tokens\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 调用示例\n",
    "words = [\"1995\", \"Amazon\", \"Morris\", \"Lisp\", \"startup\", \"software\", \"Viaweb\", \"the\", \"and\"]\n",
    "plot_semantic_evolution(words, past_key_values.mem_hidden, model, tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5262d1f2-d58b-4fcf-bc0d-f5a867baff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选一个正常的 token hidden state 做对比\n",
    "normal_hidden = model.model.embed_tokens(torch.tensor([100]).to(model.device)) \n",
    "print(f\"Normal hidden norm: {normal_hidden.norm().item()}\")\n",
    "print(f\"Mem_bank norm: {past_key_values.mem_hidden[33].norm().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f39bff-4abe-4cc1-9e50-b7745bf07753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def analyze_mem_hidden_similarity(mem_hidden_list):\n",
    "    \"\"\"\n",
    "    mem_hidden_list: 包含每层 mem_hidden 的列表 [layer_0_hidden, layer_1_hidden, ...]\n",
    "    每个元素的 shape 应该是 [hidden_size] 或 [1, hidden_size]\n",
    "    \"\"\"\n",
    "    # 1. 确保所有向量都是 2D [num_layers, hidden_size]\n",
    "    # 假设 hidden_size 是 4096 (Qwen2-7B)\n",
    "    vectors = [v.detach().cpu().float().view(-1) for v in mem_hidden_list]\n",
    "    matrix = torch.stack(vectors) # [num_layers, hidden_size]\n",
    "    \n",
    "    # 2. 计算余弦相似度矩阵\n",
    "    # S_ij = (vi · vj) / (||vi|| * ||vj||)\n",
    "    norm_matrix = F.normalize(matrix, p=2, dim=1)\n",
    "    sim_matrix = torch.mm(norm_matrix, norm_matrix.T).numpy()\n",
    "    \n",
    "    # 3. 绘图\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(sim_matrix, \n",
    "                cmap='viridis', \n",
    "                annot=False, \n",
    "                xticklabels=5, \n",
    "                yticklabels=5,\n",
    "                cbar_kws={'label': 'Cosine Similarity'})\n",
    "    \n",
    "    plt.title(\"Mem Hidden State: Cross-Layer Cosine Similarity\")\n",
    "    plt.xlabel(\"Layer Index\")\n",
    "    plt.ylabel(\"Layer Index\")\n",
    "    \n",
    "    # 特别标注第 33 层 (Layer 34)\n",
    "    # plt.axvline(x=33, color='red', linestyle='--', alpha=0.6, label='Layer 33')\n",
    "    # plt.axhline(y=33, color='red', linestyle='--', alpha=0.6)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # 4. 打印相邻层相似度 (检查演化速度)\n",
    "    print(\"Adjacent Layer Similarity (Evolution Speed):\")\n",
    "    adj_sim = np.diag(sim_matrix, k=1)\n",
    "    for i, s in enumerate(adj_sim):\n",
    "        if i >= 30: # 重点关注最后几层\n",
    "            print(f\"Layer {i} -> {i+1}: {s:.4f}\")\n",
    "            \n",
    "    return sim_matrix\n",
    "\n",
    "# 直接调用\n",
    "sim_mat = analyze_mem_hidden_similarity(past_key_values.mem_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c952c9-d6f3-4fa1-8449-fe11efe2a21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义存储容器\n",
    "internal_updates = {}\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        # 捕获 Attention 之后的残差和 (即进入 MLP 之前的状态)\n",
    "        internal_updates[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# 为最后两层注册 Hook (针对 Qwen2 结构)\n",
    "# 注意：Qwen2 的 block 结构通常是 model.layers[i]\n",
    "model.model.layers[33].post_attention_layernorm.register_forward_hook(get_activation('L33_post_attn'))\n",
    "model.model.layers[34].post_attention_layernorm.register_forward_hook(get_activation('L34_post_attn'))\n",
    "model.model.layers[35].post_attention_layernorm.register_forward_hook(get_activation('L35_post_attn'))\n",
    "\n",
    "# 执行一次推理 (使用你的 Viaweb 样本)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs,output_hidden_states=True)\n",
    "\n",
    "def detailed_component_analysis(hidden_states, internal_updates, layer_idx, pos=128):\n",
    "    # h_in: 进入该层的状态\n",
    "    h_in = hidden_states[layer_idx][0, pos, :]\n",
    "    # h_mid: Attention 之后, MLP 之前的状态\n",
    "    h_mid = internal_updates[f'L{layer_idx}_post_attn'][0, pos, :]\n",
    "    # h_out: 该层最终输出状态\n",
    "    h_out = hidden_states[layer_idx+1][0, pos, :]\n",
    "    \n",
    "    attn_update_norm = torch.norm(h_mid - h_in).item()\n",
    "    mlp_update_norm = torch.norm(h_out - h_mid).item()\n",
    "    \n",
    "    print(f\"--- Layer {layer_idx} Detailed Analysis (Pos {pos}) ---\")\n",
    "    print(f\"Attention Update Norm: {attn_update_norm:.4f}\")\n",
    "    print(f\"MLP Update Norm:       {mlp_update_norm:.4f}\")\n",
    "\n",
    "l = 34\n",
    "detailed_component_analysis(outputs.hidden_states, internal_updates, l, pos=128)\n",
    "detailed_component_analysis(outputs.hidden_states, internal_updates, l, pos=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27086242-67f4-4491-97df-98fa0025bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_hidden_deltas(hidden_states, mem_pos=128, normal_pos=-1):\n",
    "    \"\"\"\n",
    "    hidden_states: 包含所有层输出的元组\n",
    "    mem_pos: memory token 的索引\n",
    "    normal_pos: 一个普通 token (如噪声序列中的一个) 的索引做对比\n",
    "    \"\"\"\n",
    "    stats = []\n",
    "    num_layers = len(hidden_states)\n",
    "    \n",
    "    for i in range(1, num_layers):\n",
    "        # 提取当前层和前一层的向量\n",
    "        h_prev = hidden_states[i-1][0, :, :] # [seq, dim]\n",
    "        h_curr = hidden_states[i][0, :, :]\n",
    "        \n",
    "        # 计算总增量向量: delta = h_{l} - h_{l-1}\n",
    "        delta_vec = h_curr - h_prev\n",
    "        \n",
    "        # 提取特定位置的统计信息\n",
    "        mem_delta_norm = torch.norm(delta_vec[mem_pos, :]).item()\n",
    "        norm_delta_norm = torch.norm(delta_vec[normal_pos, :]).item()\n",
    "        \n",
    "        mem_base_norm = torch.norm(h_prev[mem_pos, :]).item()\n",
    "        norm_base_norm = torch.norm(h_prev[normal_pos, :]).item()\n",
    "        \n",
    "        stats.append({\n",
    "            \"Layer\": i,\n",
    "            \"Mem_Delta_Norm\": mem_delta_norm,\n",
    "            \"Normal_Delta_Norm\": norm_delta_norm,\n",
    "            \"Mem_Update_Ratio\": mem_delta_norm / mem_base_norm, # 更新强度比\n",
    "            \"Normal_Update_Ratio\": norm_delta_norm / norm_base_norm # 更新强度比\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(stats)\n",
    "    # 重点查看最后几层\n",
    "    # print(df.tail(5))\n",
    "    return df\n",
    "\n",
    "df_delta = analyze_hidden_deltas(outputs.hidden_states)\n",
    "df_delta.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a3ef39-325a-4810-ab26-dd854bfb2ded",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
